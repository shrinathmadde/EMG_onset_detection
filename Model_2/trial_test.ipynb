{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_emg_data(folder_path):\n",
    "    \"\"\"\n",
    "    Load EMG data from .set files and create binary label sequences marking onset-offset regions\n",
    "    from corresponding .json files with the same base name.\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path to the folder containing both .set and .json files\n",
    "        \n",
    "    Returns:\n",
    "        X: shape (n_epochs, time_points) - EMG signal data\n",
    "        y: shape (n_epochs, time_points) - Binary labels with 1s between onset and offset\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import json\n",
    "    from scipy import io\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Find all .set files in the folder\n",
    "    set_files = list(Path(folder_path).glob('*.set'))\n",
    "    \n",
    "    if not set_files:\n",
    "        print(f\"No .set files found in {folder_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for set_file in set_files:\n",
    "        try:\n",
    "            # Look for the corresponding JSON file with the same base name\n",
    "            json_file = set_file.with_suffix('.json')\n",
    "            \n",
    "            if not json_file.exists():\n",
    "                print(f\"Warning: No matching JSON file for {set_file.name}\")\n",
    "                continue\n",
    "            \n",
    "            # Load .set file (EMG data)\n",
    "            mat_data = io.loadmat(str(set_file), struct_as_record=True, squeeze_me=True)\n",
    "            \n",
    "            if 'data' not in mat_data or isinstance(mat_data['data'], str):\n",
    "                print(f\"Warning: No valid data in {set_file.name}\")\n",
    "                continue\n",
    "            \n",
    "            data = mat_data['data']  # shape: (channels, time_points, epochs)\n",
    "            n_channels, time_points, n_epochs = data.shape\n",
    "            \n",
    "            # Find ZM channel index\n",
    "            channel_names = []\n",
    "            zm_idx = 0  # Default to first channel\n",
    "            if 'chanlocs' in mat_data:\n",
    "                chanlocs = mat_data['chanlocs']\n",
    "                if not isinstance(chanlocs, np.ndarray):\n",
    "                    chanlocs = [chanlocs]\n",
    "                for ch in chanlocs:\n",
    "                    if hasattr(ch, 'labels'):\n",
    "                        channel_names.append(str(ch.labels))\n",
    "                if 'ZM' in channel_names:\n",
    "                    zm_idx = channel_names.index('ZM')\n",
    "            \n",
    "            # Load JSON file (onset/offset annotations)\n",
    "            with open(json_file, 'r') as f:\n",
    "                json_data = json.load(f)\n",
    "            \n",
    "            if \"ZM\" not in json_data:\n",
    "                print(f\"Warning: No ZM data in {json_file.name}\")\n",
    "                continue\n",
    "            \n",
    "            zm_json = json_data[\"ZM\"]\n",
    "            \n",
    "            # Extract onset and offset points\n",
    "            onsets = zm_json.get(\"onset\", [])\n",
    "            offsets = zm_json.get(\"offset\", [])\n",
    "            \n",
    "            # If lengths don't match, use only the matching pairs\n",
    "            min_length = min(len(onsets), len(offsets))\n",
    "            onsets = onsets[:min_length]\n",
    "            offsets = offsets[:min_length]\n",
    "            \n",
    "            # Extract ZM signal and create labels for each epoch\n",
    "            for i in range(n_epochs):\n",
    "                signal = data[zm_idx, :, i]  # (time_points,)\n",
    "                \n",
    "                # Create label array (zeros with 1s between onset and offset)\n",
    "                labels = np.zeros(time_points)\n",
    "                \n",
    "                # Mark regions between onset and offset with 1s\n",
    "                for j in range(len(onsets)):\n",
    "                    if onsets[j] is not None and offsets[j] is not None:\n",
    "                        start = max(0, onsets[j])\n",
    "                        end = min(time_points - 1, offsets[j])\n",
    "                        if start < time_points and end >= 0:\n",
    "                            labels[start:end+1] = 1\n",
    "                \n",
    "                all_data.append(signal)\n",
    "                all_labels.append(labels)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {set_file.name}: {e}\")\n",
    "    \n",
    "    X = np.array(all_data)  # (n_epochs, time_points)\n",
    "    y = np.array(all_labels)  # (n_epochs, time_points)\n",
    "    \n",
    "    print(f\"Loaded {X.shape[0]} epochs | X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "def preprocess_data(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocess the data for training\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): Input features (epochs, time_points)\n",
    "        y (numpy.ndarray): Binary mask labels (epochs, time_points)\n",
    "        test_size (float): Proportion of data to use for testing\n",
    "        random_state (int): Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_train, X_test, y_train, y_test)\n",
    "    \"\"\"\n",
    "    if len(X) == 0:\n",
    "        print(\"No data to preprocess.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Standardize the data (zero mean, unit variance)\n",
    "    mean = np.mean(X, axis=1, keepdims=True)\n",
    "    std = np.std(X, axis=1, keepdims=True)\n",
    "    X_normalized = (X - mean) / (std + 1e-8)  # Add small epsilon to prevent division by zero\n",
    "\n",
    "    # Reshape for CNN input - (samples, time_points, channels)\n",
    "    X_reshaped = X_normalized[:, :, np.newaxis]\n",
    "\n",
    "    # Ensure y is in the right format for sequence prediction\n",
    "    # For binary mask prediction, we keep y as a sequence of the same length as input\n",
    "    y = y.astype(np.float32)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_reshaped, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    print(f\"Training set: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"Test set: X={X_test.shape}, y={y_test.shape}\")\n",
    "    \n",
    "    # Calculate class imbalance metrics\n",
    "    train_masked_percentage = (np.sum(y_train > 0) / y_train.size) * 100\n",
    "    test_masked_percentage = (np.sum(y_test > 0) / y_test.size) * 100\n",
    "    print(f\"Training set masked points: {train_masked_percentage:.2f}%\")\n",
    "    print(f\"Test set masked points: {test_masked_percentage:.2f}%\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing training samples with onset and offset markers...\n",
      "Warning: No valid data in EMGfast_01.set\n",
      "Warning: No valid data in EMGfast_10.set\n",
      "Loaded 800 epochs | X shape: (800, 435), y shape: (800, 435)\n",
      "âœ… Saved 800 training plots with valid onsets/offsets to: C:\\EMG_onset_detection\\LOL_project\\results\\train_img\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "# Keep your load_emg_data function as previously defined\n",
    "# from data_loader import load_emg_data, preprocess_data\n",
    "\n",
    "print(\"Visualizing training samples with onset and offset markers...\")\n",
    "\n",
    "train_img_dir = os.path.join(r\"C:\\EMG_onset_detection\\LOL_project\\results\", \"train_img\")\n",
    "os.makedirs(train_img_dir, exist_ok=True)\n",
    "\n",
    "threshold = 0  # Label threshold to determine if an onset exists\n",
    "\n",
    "saved_count = 0\n",
    "X_train, y_train = load_emg_data(r\"C:\\EMG_onset_detection\\LOL_project\\epoched_EMG_data\")\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    signal = X_train[i].squeeze()\n",
    "    label = y_train[i].squeeze()\n",
    "    \n",
    "    # Only save if a valid label (onset/offset region) is present\n",
    "    if np.max(label) > threshold:\n",
    "        # Find onset and offset indices\n",
    "        # Onset is the first point where label becomes 1\n",
    "        onset_indices = np.where(np.diff(np.concatenate(([0], label))) == 1)[0]\n",
    "        # Offset is the first point where label becomes 0 after being 1\n",
    "        offset_indices = np.where(np.diff(np.concatenate((label, [0]))) == -1)[0]\n",
    "        \n",
    "        plt.figure(figsize=(10, 3))\n",
    "        plt.plot(signal, label=\"EMG Signal\", linewidth=1)\n",
    "        \n",
    "        # Plot all onset-offset pairs with different colors\n",
    "        colors = ['r', 'g', 'b', 'm', 'c', 'y']\n",
    "        for j, (onset, offset) in enumerate(zip(onset_indices, offset_indices)):\n",
    "            color = colors[j % len(colors)]\n",
    "            plt.axvline(onset, color=color, linestyle='--', \n",
    "                       label=f\"Onset {j+1}: {onset}\")\n",
    "            plt.axvline(offset, color=color, linestyle=':',\n",
    "                       label=f\"Offset {j+1}: {offset}\")\n",
    "            \n",
    "            # Add shaded region between onset and offset\n",
    "            plt.axvspan(onset, offset, alpha=0.2, color=color)\n",
    "        \n",
    "        plt.title(f\"Training Epoch {i}\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(train_img_dir, f\"train_epoch_{i}.png\"))\n",
    "        plt.close()\n",
    "        saved_count += 1\n",
    "\n",
    "print(f\"âœ… Saved {saved_count} training plots with valid onsets/offsets to: {train_img_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 435), (800, 435))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape,X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level keys in the JSON file:\n",
      "sub_id\n",
      "ZM\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to your .json file\n",
    "json_path = r\"C:\\EMG_onset_detection\\LOL_project\\epoched_EMG_data\\EMGfast_10.json\"\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# If it's a dictionary, get all keys\n",
    "keys = list(data.keys())\n",
    "\n",
    "print(\"Top-level keys in the JSON file:\")\n",
    "for key in keys:\n",
    "    print(key)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
