{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e343ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting DEMANN Prediction ---\n",
      "Output directory is set to: 'demann_results_continuous_data'\n",
      "Analyzing burst labels from C:\\EMG_onset_detection\\LOL_project\\continuous_prediction_data\\all_burst_labels.json...\n",
      "NOTE: Using placeholder data. You must import and call 'analyze_burst_labels'.\n",
      "Total items: 4628\n",
      "Items with valid labels: 0\n",
      "Items without valid labels: 4628\n",
      "NOTE: Using a placeholder transform. You must import 'SignalNormalize'.\n",
      "Dataset created with 0 items\n",
      "NOTE: Using a placeholder dataset. You must import 'EMGDataset'.\n",
      "Loading model from C:\\EMG_onset_detection\\LOL_project\\demann_results\\model\\demann_model.h5...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,027</span> (4.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,027\u001b[0m (4.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> (4.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,025\u001b[0m (4.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signalshape (0,)\n",
      "Extracting DEMANN features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 507\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# Make predictions using the DEMANN model\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# Ensure the 'predict_with_demann' function is defined or imported\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting predictions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 507\u001b[0m \u001b[43mpredict_with_demann\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Prediction Complete ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction results have been saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 276\u001b[0m, in \u001b[0;36mpredict_with_demann\u001b[1;34m(model, dataset, output_dir, fs, batch_size)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignalshape\u001b[39m\u001b[38;5;124m\"\u001b[39m,all_signals\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting DEMANN features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 276\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_demann_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_signals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m n_samples, n_windows, feature_dim \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# Reshape features for prediction\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 148\u001b[0m, in \u001b[0;36mextract_demann_features\u001b[1;34m(signals, fs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpywt\u001b[39;00m\n\u001b[0;32m    147\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(signals)\n\u001b[1;32m--> 148\u001b[0m time_points \u001b[38;5;241m=\u001b[39m \u001b[43msignals\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    149\u001b[0m all_features \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Butterworth filter setup with corrected frequency normalization\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Import DEMANN modules - make sure these are available in your path\n",
    "from model import create_demann_model\n",
    "from evaluation import post_process_predictions, find_events, evaluate_events\n",
    "\n",
    "class SignalNormalize:\n",
    "    \"\"\"Transform to normalize signals\"\"\"\n",
    "    def __call__(self, signal):\n",
    "        # Z-score normalization (zero mean, unit variance)\n",
    "        mean = np.mean(signal)\n",
    "        std = np.std(signal)\n",
    "        return (signal - mean) / (std + 1e-8)  # Add small epsilon to prevent division by zero\n",
    "\n",
    "class EMGDataset(Dataset):\n",
    "    \"\"\"Dataset for EMG signals with onset/offset labels\"\"\"\n",
    "    \n",
    "    def __init__(self, data_items, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_items: List of dictionaries with npz_path and json_path\n",
    "            transform: Optional transform to apply to the data\n",
    "        \"\"\"\n",
    "        self.data_items = data_items\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Default signal length\n",
    "        self.signal_length = 435  # Standard epoch length\n",
    "        \n",
    "        print(f\"Dataset created with {len(data_items)} items\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data_items[idx]\n",
    "        \n",
    "        # Get file paths\n",
    "        npz_path = item['npz_path']\n",
    "        json_path = item['json_path']\n",
    "        \n",
    "        # Extract filename for plotting\n",
    "        filename = os.path.basename(npz_path).split('_emg.npz')[0]\n",
    "        \n",
    "        # Try to load the signal data\n",
    "        try:\n",
    "            data = np.load(npz_path, allow_pickle=True)\n",
    "            \n",
    "            # Access signal data following the working approach\n",
    "            signal_dict = data[\"signal\"].item()  # Convert to dictionary\n",
    "            signal = signal_dict[\"signal\"]\n",
    "            \n",
    "            # Make sure signal is a numpy array of float32\n",
    "            signal = np.asarray(signal, dtype=np.float32)\n",
    "            \n",
    "            # If signal is not the right shape, resize it\n",
    "            if len(signal) != self.signal_length:\n",
    "                temp_signal = np.zeros(self.signal_length, dtype=np.float32)\n",
    "                copy_length = min(len(signal), self.signal_length)\n",
    "                temp_signal[:copy_length] = signal[:copy_length]\n",
    "                signal = temp_signal\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading signal from {npz_path}: {e}\")\n",
    "            # If we can't load the signal, use zeros\n",
    "            signal = np.zeros(self.signal_length, dtype=np.float32)\n",
    "        \n",
    "        # Create binary mask (0 = no activity, 1 = activity)\n",
    "        mask = np.zeros(self.signal_length, dtype=np.float32)\n",
    "        \n",
    "        # Load label data if available\n",
    "        if item.get('has_valid_label', False):\n",
    "            try:\n",
    "                with open(json_path, 'r') as f:\n",
    "                    label_data = json.load(f)\n",
    "                \n",
    "                # Check for onset/offset directly in the label_data\n",
    "                onset = label_data.get('onset')\n",
    "                offset = label_data.get('offset')\n",
    "                \n",
    "                if onset is not None and offset is not None:\n",
    "                    # Make sure onset/offset are within valid range\n",
    "                    if 0 <= onset < self.signal_length and 0 <= offset < self.signal_length:\n",
    "                        mask[onset:offset+1] = 1.0\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading label from {json_path}: {e}\")\n",
    "                # If we can't load the labels, keep zeros\n",
    "                pass\n",
    "        \n",
    "        # Apply any transformations\n",
    "        if self.transform:\n",
    "            signal = self.transform(signal)\n",
    "        \n",
    "        return signal, mask, filename\n",
    "\n",
    "def analyze_burst_labels(json_path):\n",
    "    \"\"\"\n",
    "    Analyze the all_burst_labels.json file to find valid/invalid labels\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to the all_burst_labels.json file\n",
    "        \n",
    "    Returns:\n",
    "        valid_data: List of items with valid labels\n",
    "        invalid_data: List of items with invalid labels\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        all_data = json.load(f)\n",
    "    \n",
    "    valid_data = []\n",
    "    invalid_data = []\n",
    "    \n",
    "    for item in all_data:\n",
    "        if item.get('has_valid_label', False):\n",
    "            valid_data.append(item)\n",
    "        else:\n",
    "            invalid_data.append(item)\n",
    "    \n",
    "    print(f\"Total items: {len(all_data)}\")\n",
    "    print(f\"Items with valid labels: {len(valid_data)}\")\n",
    "    print(f\"Items without valid labels: {len(invalid_data)}\")\n",
    "    \n",
    "    return valid_data, invalid_data\n",
    "\n",
    "def extract_demann_features(signals, fs=1000):\n",
    "    \"\"\"\n",
    "    Extract the features needed for DEMANN: LE, RMS, and CWT.\n",
    "    \n",
    "    Args:\n",
    "        signals: EMG signal data, shape (n_samples, time_points)\n",
    "        fs: Sampling frequency in Hz (default: 1000)\n",
    "        \n",
    "    Returns:\n",
    "        all_features: Combined features for DEMANN, shape (n_samples, n_windows, feature_dim)\n",
    "    \"\"\"\n",
    "    from scipy import signal as signal_processing\n",
    "    import pywt\n",
    "    \n",
    "    n_samples = len(signals)\n",
    "    time_points = signals[0].shape[0]\n",
    "    all_features = []\n",
    "    \n",
    "    # Butterworth filter setup with corrected frequency normalization\n",
    "    nyquist = fs / 2\n",
    "    \n",
    "    # Ensure filter frequencies are in the valid range (0 < Wn < 1)\n",
    "    low_freq = min(10/nyquist, 0.99)  # Ensure it's less than 1\n",
    "    high_freq = min(500/nyquist, 0.99)  # Ensure it's less than 1\n",
    "    \n",
    "    # Make sure the frequencies are above 0\n",
    "    low_freq = max(low_freq, 0.001)\n",
    "    high_freq = max(high_freq, 0.001)\n",
    "    \n",
    "    # If frequencies are too close, adjust them\n",
    "    if abs(high_freq - low_freq) < 0.001:\n",
    "        high_freq = min(low_freq + 0.1, 0.99)\n",
    "    \n",
    "    for i in tqdm(range(n_samples), desc=\"Extracting features\"):\n",
    "        emg_signal = signals[i]  # (time_points,)\n",
    "        \n",
    "        try:\n",
    "            # Apply band-pass filter (10-500 Hz)\n",
    "            b, a = signal_processing.butter(2, [low_freq, high_freq], btype='bandpass')\n",
    "            filtered_emg = signal_processing.filtfilt(b, a, emg_signal)\n",
    "            \n",
    "            # Ensure non-negative signal (full-wave rectification)\n",
    "            filtered_emg = np.abs(filtered_emg)\n",
    "            \n",
    "            # 1. Linear Envelope (LE) - Low-pass filter at 5 Hz\n",
    "            le_freq = min(5/nyquist, 0.99)\n",
    "            le_freq = max(le_freq, 0.001)  # Ensure it's above 0\n",
    "            b_le, a_le = signal_processing.butter(2, le_freq, btype='lowpass')\n",
    "            le = signal_processing.filtfilt(b_le, a_le, filtered_emg)\n",
    "            \n",
    "            # 2. Root Mean Square (RMS) with 60-sample sliding window\n",
    "            window_size = min(60, len(filtered_emg) // 10)  # Ensure window is not too large\n",
    "            rms = np.zeros_like(filtered_emg)\n",
    "            \n",
    "            # Pad signal for edge handling\n",
    "            padded_emg = np.pad(filtered_emg, (window_size//2, window_size//2), mode='edge')\n",
    "            \n",
    "            for j in range(len(filtered_emg)):\n",
    "                window = padded_emg[j:j+window_size]\n",
    "                rms[j] = np.sqrt(np.mean(window**2))\n",
    "            \n",
    "            # 3. Continuous Wavelet Transform (CWT)\n",
    "            scales = np.arange(1, 7)  # 6 levels of decomposition as in the paper\n",
    "            cwt_coeffs, _ = pywt.cwt(filtered_emg, scales, 'morl')\n",
    "            \n",
    "            # Compute scalogram (square of absolute CWT coefficients)\n",
    "            cwt_scalogram = np.abs(cwt_coeffs)**2\n",
    "            \n",
    "            # Reduce dimensionality by averaging across scales\n",
    "            cwt_feature = np.mean(cwt_scalogram, axis=0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing signal {i}: {e}\")\n",
    "            # Use zeros if filter fails\n",
    "            filtered_emg = np.abs(emg_signal)\n",
    "            le = filtered_emg\n",
    "            rms = filtered_emg\n",
    "            cwt_feature = filtered_emg\n",
    "        \n",
    "        # Min-max normalization for each feature\n",
    "        def min_max_normalize(x):\n",
    "            min_val = np.min(x)\n",
    "            max_val = np.max(x)\n",
    "            if max_val - min_val < 1e-8:\n",
    "                return np.zeros_like(x)\n",
    "            return (x - min_val) / (max_val - min_val + 1e-10)\n",
    "        \n",
    "        le_norm = min_max_normalize(le)\n",
    "        rms_norm = min_max_normalize(rms)\n",
    "        cwt_norm = min_max_normalize(cwt_feature)\n",
    "        \n",
    "        # Create sliding windows with size=10 for DEMANN\n",
    "        window_size = 10\n",
    "        n_windows = time_points - window_size + 1\n",
    "        sample_features = []\n",
    "        \n",
    "        for j in range(n_windows):\n",
    "            # Create concatenated feature vector for this window\n",
    "            window_features = np.concatenate([\n",
    "                le_norm[j:j+window_size],\n",
    "                rms_norm[j:j+window_size],\n",
    "                cwt_norm[j:j+window_size]\n",
    "            ])\n",
    "            sample_features.append(window_features)\n",
    "        \n",
    "        all_features.append(sample_features)\n",
    "    \n",
    "    return np.array(all_features)\n",
    "\n",
    "def predict_with_demann(model, dataset, output_dir=\"predictions\", fs=1000, batch_size=32):\n",
    "    \"\"\"\n",
    "    Make predictions with a trained DEMANN model\n",
    "    \n",
    "    Args:\n",
    "        model: Trained DEMANN model\n",
    "        dataset: EMGDataset instance\n",
    "        output_dir: Directory to save predictions\n",
    "        fs: Sampling frequency in Hz\n",
    "        batch_size: Batch size for DataLoader\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with predictions and evaluation metrics\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load all signals and masks from the dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    all_signals = []\n",
    "    all_masks = []\n",
    "    all_filenames = []\n",
    "    \n",
    "    for signals, masks, filenames in tqdm(dataloader, desc=\"Loading dataset\"):\n",
    "        all_signals.extend(signals.numpy())\n",
    "        all_masks.extend(masks.numpy())\n",
    "        all_filenames.extend(filenames)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_signals = np.array(all_signals)\n",
    "    all_masks = np.array(all_masks)\n",
    "    \n",
    "    # Extract DEMANN features\n",
    "    print(\"signalshape\",all_signals.shape)\n",
    "    print(\"Extracting DEMANN features...\")\n",
    "    features = extract_demann_features(all_signals, fs)\n",
    "    n_samples, n_windows, feature_dim = features.shape\n",
    "    \n",
    "    # Reshape features for prediction\n",
    "    X_windows = features.reshape(-1, feature_dim)\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"Making predictions...\")\n",
    "    y_pred_prob = model.predict(X_windows)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Reshape predictions by sample\n",
    "    predictions = []\n",
    "    for i in range(n_samples):\n",
    "        sample_pred = y_pred[i*n_windows:(i+1)*n_windows]\n",
    "        \n",
    "        # Post-process to remove short activations\n",
    "        processed_pred = post_process_predictions(sample_pred, min_duration=60)\n",
    "        \n",
    "        # Resize prediction to match original signal length\n",
    "        signal_length = all_masks[i].shape[0]\n",
    "        resized_pred = np.zeros(signal_length, dtype=int)\n",
    "        copy_length = min(len(processed_pred), signal_length)\n",
    "        resized_pred[:copy_length] = processed_pred[:copy_length]\n",
    "        \n",
    "        predictions.append(resized_pred)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Create results directory\n",
    "    results_dir = os.path.join(output_dir, 'results')\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Save individual prediction plots\n",
    "    plots_dir = os.path.join(output_dir, 'plots')\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Evaluate metrics and save plots\n",
    "    all_metrics = {\n",
    "        'onset': {'precision': [], 'recall': [], 'f1': [], 'mae': []},\n",
    "        'offset': {'precision': [], 'recall': [], 'f1': [], 'mae': []}\n",
    "    }\n",
    "    \n",
    "    tolerance_samples = int(0.1 * fs)  # 100ms tolerance\n",
    "    \n",
    "    print(\"Evaluating predictions and saving plots...\")\n",
    "    for i in tqdm(range(n_samples)):\n",
    "        # Save prediction as NPZ file with onset and offset information\n",
    "        pred_onset_indices, pred_offset_indices = find_events(predictions[i])\n",
    "        \n",
    "        # Create a dictionary to save\n",
    "        pred_data = {\n",
    "            'signal': all_signals[i],\n",
    "            'ground_truth': all_masks[i],\n",
    "            'prediction': predictions[i],\n",
    "            'onsets': pred_onset_indices,\n",
    "            'offsets': pred_offset_indices\n",
    "        }\n",
    "        \n",
    "        np.savez(\n",
    "            os.path.join(results_dir, f\"{all_filenames[i]}_prediction.npz\"),\n",
    "            **pred_data\n",
    "        )\n",
    "        \n",
    "        # Find events\n",
    "        gt_onset_indices, gt_offset_indices = find_events(all_masks[i])\n",
    "        \n",
    "        # Evaluate only if there are ground truth labels\n",
    "        if len(gt_onset_indices) > 0 and len(gt_offset_indices) > 0:\n",
    "            # Evaluate onset detection\n",
    "            onset_metrics = evaluate_events(\n",
    "                pred_onset_indices, gt_onset_indices, tolerance_samples, fs\n",
    "            )\n",
    "            \n",
    "            # Evaluate offset detection\n",
    "            offset_metrics = evaluate_events(\n",
    "                pred_offset_indices, gt_offset_indices, tolerance_samples, fs\n",
    "            )\n",
    "            \n",
    "            # Store metrics\n",
    "            for key in all_metrics['onset']:\n",
    "                all_metrics['onset'][key].append(onset_metrics[key])\n",
    "                all_metrics['offset'][key].append(offset_metrics[key])\n",
    "        \n",
    "        # Create and save plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Calculate time array (in seconds)\n",
    "        time = np.arange(len(all_signals[i])) / fs\n",
    "        \n",
    "        # Plot signal and regions\n",
    "        plt.plot(time, all_signals[i], 'b-', alpha=0.7, label='EMG Signal')\n",
    "        \n",
    "        # Plot ground truth (if available)\n",
    "        if np.any(all_masks[i] > 0):\n",
    "            for onset in gt_onset_indices:\n",
    "                plt.axvline(x=onset/fs, color='g', linestyle='--', alpha=0.7, label='GT Onset' if onset == gt_onset_indices[0] else \"\")\n",
    "            for offset in gt_offset_indices:\n",
    "                plt.axvline(x=offset/fs, color='r', linestyle='--', alpha=0.7, label='GT Offset' if offset == gt_offset_indices[0] else \"\")\n",
    "            plt.fill_between(time, 0, 1, where=all_masks[i] > 0, color='g', alpha=0.2, transform=plt.gca().get_xaxis_transform(), label='GT Activity')\n",
    "        \n",
    "        # Plot predictions\n",
    "        for onset in pred_onset_indices:\n",
    "            plt.axvline(x=onset/fs, color='m', linestyle='-', alpha=0.7, label='Pred Onset' if onset == pred_onset_indices[0] else \"\")\n",
    "        for offset in pred_offset_indices:\n",
    "            plt.axvline(x=offset/fs, color='c', linestyle='-', alpha=0.7, label='Pred Offset' if offset == pred_offset_indices[0] else \"\")\n",
    "        plt.fill_between(time, 0, 1, where=predictions[i] > 0, color='b', alpha=0.2, transform=plt.gca().get_xaxis_transform(), label='Pred Activity')\n",
    "        \n",
    "        plt.title(f\"EMG Onset/Offset Detection - {all_filenames[i]}\")\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plots_dir, f\"{all_filenames[i]}_plot.png\"))\n",
    "        plt.close()\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_metrics = {\n",
    "        'onset': {\n",
    "            'precision': np.mean([x for x in all_metrics['onset']['precision'] if not np.isnan(x)]),\n",
    "            'recall': np.mean([x for x in all_metrics['onset']['recall'] if not np.isnan(x)]),\n",
    "            'f1': np.mean([x for x in all_metrics['onset']['f1'] if not np.isnan(x)]),\n",
    "            'mae': np.mean([x for x in all_metrics['onset']['mae'] if x != float('inf')])\n",
    "        },\n",
    "        'offset': {\n",
    "            'precision': np.mean([x for x in all_metrics['offset']['precision'] if not np.isnan(x)]),\n",
    "            'recall': np.mean([x for x in all_metrics['offset']['recall'] if not np.isnan(x)]),\n",
    "            'f1': np.mean([x for x in all_metrics['offset']['f1'] if not np.isnan(x)]),\n",
    "            'mae': np.mean([x for x in all_metrics['offset']['mae'] if x != float('inf')])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save metrics to file\n",
    "    with open(os.path.join(output_dir, 'metrics.txt'), 'w') as f:\n",
    "        f.write(\"Onset Detection Metrics:\\n\")\n",
    "        f.write(f\"Precision: {avg_metrics['onset']['precision']:.4f}\\n\")\n",
    "        f.write(f\"Recall: {avg_metrics['onset']['recall']:.4f}\\n\")\n",
    "        f.write(f\"F1-Score: {avg_metrics['onset']['f1']:.4f}\\n\")\n",
    "        f.write(f\"MAE: {avg_metrics['onset']['mae']:.2f} ms\\n\\n\")\n",
    "        \n",
    "        f.write(\"Offset Detection Metrics:\\n\")\n",
    "        f.write(f\"Precision: {avg_metrics['offset']['precision']:.4f}\\n\")\n",
    "        f.write(f\"Recall: {avg_metrics['offset']['recall']:.4f}\\n\")\n",
    "        f.write(f\"F1-Score: {avg_metrics['offset']['f1']:.4f}\\n\")\n",
    "        f.write(f\"MAE: {avg_metrics['offset']['mae']:.2f} ms\\n\")\n",
    "    \n",
    "    print(\"\\nOnset Detection Metrics:\")\n",
    "    print(f\"Precision: {avg_metrics['onset']['precision']:.4f}\")\n",
    "    print(f\"Recall: {avg_metrics['onset']['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {avg_metrics['onset']['f1']:.4f}\")\n",
    "    print(f\"MAE: {avg_metrics['onset']['mae']:.2f} ms\")\n",
    "    \n",
    "    print(\"\\nOffset Detection Metrics:\")\n",
    "    print(f\"Precision: {avg_metrics['offset']['precision']:.4f}\")\n",
    "    print(f\"Recall: {avg_metrics['offset']['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {avg_metrics['offset']['f1']:.4f}\")\n",
    "    print(f\"MAE: {avg_metrics['offset']['mae']:.2f} ms\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'metrics': avg_metrics\n",
    "    }\n",
    "\n",
    "# DEMANN for EMG Onset/Offset Detection - Prediction Script for Jupyter Notebook\n",
    "\n",
    "\n",
    "\n",
    "# Path to the all_burst_labels.json file\n",
    "json_labels_path = r'C:\\EMG_onset_detection\\LOL_project\\continuous_prediction_data\\all_burst_labels.json' \n",
    "\n",
    "# Directory to save prediction results\n",
    "output_dir = 'demann_results_continuous_data'\n",
    "\n",
    "# Sampling frequency of the EMG data in Hz\n",
    "sampling_freq = 1000\n",
    "\n",
    "# Path to the pre-trained DEMANN model\n",
    "model_path = 'C:\\EMG_onset_detection\\LOL_project\\demann_results\\model\\demann_model.h5' \n",
    "\n",
    "# Batch size for processing\n",
    "batch_size = 32\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Main Execution Logic\n",
    "# ==============================================================================\n",
    "# --- This section contains the core logic from your original main() function ---\n",
    "\n",
    "print(\"--- Starting DEMANN Prediction ---\")\n",
    "\n",
    "# Check if the model file exists\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Error: Model file '{model_path}' not found!\")\n",
    "    # In a notebook, you might want to stop execution here.\n",
    "    # You can raise an exception or simply not proceed.\n",
    "else:\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Output directory is set to: '{output_dir}'\")\n",
    "    \n",
    "    # Analyze burst labels from the specified JSON file\n",
    "    # Ensure the 'analyze_burst_labels' function is defined or imported\n",
    "    print(f\"Analyzing burst labels from {json_labels_path}...\")\n",
    "    # valid_data, invalid_data = analyze_burst_labels(json_labels_path)\n",
    "    \n",
    "    # For this example, we'll create placeholder data.\n",
    "    # Replace this with your actual call to analyze_burst_labels.\n",
    "    print(\"NOTE: Using placeholder data. You must import and call 'analyze_burst_labels'.\")\n",
    " \n",
    "    valid_data, invalid_data = analyze_burst_labels(json_labels_path)\n",
    "    # You can decide whether to use only valid data or all data\n",
    "    data_to_use = valid_data\n",
    "    \n",
    "    # Set up the signal transformation\n",
    "    # Ensure the 'SignalNormalize' class is defined or imported\n",
    "    transform = SignalNormalize()\n",
    "    print(\"NOTE: Using a placeholder transform. You must import 'SignalNormalize'.\")\n",
    "  \n",
    "    \n",
    "    # Create the dataset for prediction\n",
    "    # Ensure the 'EMGDataset' class is defined or imported\n",
    "    dataset = EMGDataset(data_to_use, transform=transform)\n",
    "    print(\"NOTE: Using a placeholder dataset. You must import 'EMGDataset'.\")\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    model.summary()\n",
    "    \n",
    "    # Make predictions using the DEMANN model\n",
    "    # Ensure the 'predict_with_demann' function is defined or imported\n",
    "    print(\"Starting predictions...\")\n",
    "    predict_with_demann(\n",
    "        model,\n",
    "        dataset,\n",
    "        output_dir=output_dir,\n",
    "        fs=sampling_freq,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    print(f\"--- Prediction Complete ---\")\n",
    "    print(f\"Prediction results have been saved to {output_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
